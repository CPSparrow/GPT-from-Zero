## 当前计划

最开始的规模应该向gpt-2看齐(也许吧)。

0. 训练tokenizer(BPE)~~和embedding(?)~~
    - 测试```load_jsonl```函数
    - 考虑BBPE？
1. 基础Transfomer
    - 使用torch，或者transformers?
2. 优化的transformer：RoPE,flash attention,swi激活函数，也许还有其他
    - 多卡训练
    - 更大模型
    - accelerate库
3. MoE架构，DS MoE？
## 数据来源(部分不考虑)
- 传统文化： 还需清洗，懒得搞就不搞了
- **基础语料**：目前就用这个了，质量比较高的中文语料，网安平台下载的
- cci: 来源同上，还没下载来
- oscar： 来自mnbvc的crawler文件夹
- ~~[CLUECorpus2020](https://github.com/CLUEbenchmark/CLUECorpus2020)~~ 通过百度网盘提供，所以不想搞
- [liwu/MNBVC](https://huggingface.co/datasets/liwu/MNBVC) 超大规模数据集，oscar是common crawl的清洗，可以作为初步预训练的语料。
- [c4](https://hf-mirror.com/datasets/allenai/c4/tree/main/en) 可以作为英文训练
- [smoltalk](https://opencsg.com/datasets/OpenCSG/smoltalk_chinese/files/main/data) 作为sft语料